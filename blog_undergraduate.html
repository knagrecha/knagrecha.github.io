
<!DOCTYPE html>
<html>
    <head>
        <title>
            Kabir Nagrecha
        </title>
        <meta charset="UTF-8">
        <link rel="stylesheet" href="./public/css/main.css">

    </head>

    <body>
            <div class ="header_block">
                <div class="profile_block" id="profile_block">
                    <img src="./public/images/broken_pencil.jpeg" alt="A broken pencil" width="85%">
                    <p style="margin-top: 1px; font-family:Helvetica; font-size: 22px; color:darkred">Reflections on an Undergraduate Degree</p>
                    <a style="margin-top: 2px; font-size: 18px" href="mailto:knagrech@ucsd.edu"> Contact: knagrech@ucsd.edu </a>

                </div>
                <div class="profile_text" id="profile_text">

                      <p style="margin-top: 10px; font-size: 18px; text-align: left; line-height:1.8em">
                        In the October 1925 edition of the Southwest Review, there were two articles in particular
                        that caught my attention. The first, an examination of how undergraduates in the United States
                        and Europe differ. The second, a complaint against rigid examinations in academia,
                        describing how such an approach discourages novel thought among students. The common thread
                        between the two is the focus on one central issue -  a lack of <em>preparedness</em> among
                        undergradutes.<br/> <br/>

                        Nearly a century later, I believe that this problem persists. At least in the field of computer science,
                        undergraduate coursework does not prepare students sufficiently for industry. Industry-oriented courses
                        are extremely rare, and in my opinion, they should be a mandatory part of the curriculum in all computer
                        science degrees.
                        <br/> <br/>

                        In this article, I examine what steps are taken within the undergraduate degree
                        to prepare students for industry, and why I believe these steps are insufficient. <br/> <br/>
                      </p>
                </div>
            </div>


        <div class="blog_text" id="blog_body">

              <span style="font-size: 24px;"> Industry </span> <br/><br/>

              <p>
                There is a strange, foreign place, far beyond the . Its name is discussed with
                both a deep respect as well as a strange fear. It is the great other, the unknown. It is Industry.
                Industry! Even the name seems to hold power. Industry, where things are done <em> better </em>. Industry,
                where efficiency is key. Industry, where progress is the standard and nothing is ever done wrong.
              </p>
              <p>
                But also, a place where the air is rife with stagnation. Where freedom and independence are a long-forgotten
                dream. Where hard-working developers take the role of Atlas, holding up the tremendous weight of a rotting
                bureaucracy.
              </p>

              <p>
                It's a strange doublethink, contradictory in its very nature. But this is the worldview of the undergraduate.
                I have heard both opinions expressed during my degree, often by the same person! The contradictions can be explained,
                of course. Industry is not a monolith, and companies differ greatly in their cultures. Yet often such
                nuances are lost on the fresh undergraduate looking to absorb all the information they possibly can.
              </p>
              <p>
                Up until the time they graduate, most computer science students will be confused on both the nature and demands
                of work in industry. The only exceptions are those students who have interned or worked at a company in the past.
                No wonder internships are so valued in this space. But as for those students who are <em>not</em> in a position to
                work as an intern, what chance do they have to learn about the working environment? How can they better understand
                what it means to be a professional, rather than a student?
              </p> <br/> <br/>



          <span style="font-size: 24px;">The Undergraduate</span> <br/><br/>
              <p>
                Most computer science undergraduates will obtain sufficient technical preparation from the curriculum that
                their university prescribes. What they lack is preparation in the softer skills - the ability to collaborate,
                to work independently, to make their own decisions. Most courses will provide rigid targets and set workflows
                that the student must abide by. There is little room for movement and novel thought. They never have the opportunity
                to learn how to make their own decisions or take ownership of their work. Independence, adaptability, and responsibility
                are not part of the curriculum. Yet are these not the very skills that characterize the successful professional?
              </p>
              <p>
                An undergraduate will likely spend their first year or two covering basic technical fundamentals.
                In the latter years of their degree, the studentwill focus on acquiring
                a stronger technical base in one specialization or another (databases, ML, systems, etc).
              </p>
              <p>
                I do not mean to say that the topics covered in such a path are not useful. The knowledge and skills provided in these
                courses can be invaluable. However, it seems to me that the definition of "fundamentals" within a computer science degree
                are far too narrow. If we consider data structures and algorithms to be fundamental parts of a computer science education,
                then why not technical communication and software engineering methodologies? The justification for including one topic or another
                is usually that this topic will turn out to be critical to the student's future career. They will end up using data structures and
                algorithms in their work, and so they must learn these "fundamentals". Under this same reasoning, should we not have courses
                focused on popular software development methodologies? Should we not at least discuss the topics of proper documentation,
                testing techniques, and working towards a product spec?
              </p>
              <p>
                First, as a student, you are still being asked to stick to a
                certain workflow or toolset. There is no possibility of "true" independence in what you are being asked to do,
                because the goals must reflect the course aims. For example, let us suppose that you are given an assignment
                to develop a website. Yes, you have the "freedom" to develop that site as you choose, but you will
                be graded on more than just output! You will be asked to avoid using 3rd party tools, to avoid certain
                web technologies (or else be required to use another). This is a far cry from the results-oriented workflow
                present in a professional setting. Most of the time, you are trusted to be sufficiently competent that you do
                not need the hand-holding that is <em>necessary</em> for a structured course.
              </p>
              <p> <br/> <br/>








</html>
